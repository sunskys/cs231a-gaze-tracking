\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{A Low Cost Eye-tracking Interface to Help the Physically
  Impaired Users}

\author{Thomas Karpat (SCPD)\\
Stanford University \\
{\tt\small tkarpati@stanford.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Shankar Thadhalani (SCPD)\\
Stanford University \\
{\tt\small shankart@stanford.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
This paper presents a low cost alternative to traditional eye-tracking
interface to help physically impaired users interact. We propose to
implement eye-tracking system utilizing a single camera. We present a
Convolutional Neural Networks based approach which eliminates the
necessity of external light source as required by existing solution.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
We propose to implement an eye-tracking
system utilizing a single camera. Gaze tracking has numerous
application in human computer interfaces from use as input for the
impaired[1], to determination of region of interest for augmented
reality and image processing. Our system would use a camera on a
laptop and determine the target of the users gaze. This would then be
used to move a cursor around a graphical keyboard on the display which
could be used for input. Our design results in a multi-segment
system. We propose to first segment the face from the image. Then the
left and right eyes are each segmented from the face. The eyes are
then fed into a trained neural network to determine the location of
focus for the user. We will examine work that has already been done in
the past on gaze tracking to build upon. The literature contains
systems from strictly classical approaches to systems that are built
on neural networks for the processing[3][4][5][6][7][8]. We hope to be
able to use a combination of the two to achieve good results. While
existing commercial solutions use external light source and additional
hardware to implement eye tracking, we expect to utilize openly
available gaze eye-tracking dataset to eliminate the requirement for
any additional hardware.

\section{Datasets}
We will use the Point of Gaze Eye-Tracking Dataset from the University
of Texas at Arlington[2]. This provides a mapping from a video of eyes
to locations in 3D space where the gaze of a user is focusing their
attention. The dataset consists of data captured from 20 subjects. The
dataset for each subject consists of a video of the user’s eye,
transform matrices that relate the user’s eye and the screen of a
display, and the target location on the screen that the subject is
looking at. There are 6 scenarios for each of the 20 users. The
transform and the target locations are provided per frame of the
video. We also plan on using the comprehensive head pose and gaze
database[4] to infer correlation between the pose of the head and the
target that the subject is looking at.

We hope to run the system in a real-time manner and compare our
results to expected locations on the display. In this setup data that
we capture from the camera connected to the system will be used as our
system inputs and data to process.

\section{Methods}
We propose that the system will capture video from camera available on
most laptops as the input capture device. The images from the video
can be used to determine the face of the user using the Viola-Jones
face recognition algorithm in OpenCV. The face itself can be extracted
from the video and the eyes can be isolated from the face. The images
of the eyes are fed into a neural network to determine the location of
the gaze. The neural network is trained by the data from the Point of
Gase dataset. This can then provide a mapping from the eye to the
location in space that the user is looking. This information can then
be used to perform various tasks such as selecting keys from an
on-screen keyboard extended to move a bounding box around the screen
for an item of interest.

We propose to implement the system in Python 2.7 using the OpenCV
libraries for image processing tasks. The neural network is
implemented in TensorFlow (also in Python). The use of OpenCV gives us
an easy hardware abstraction layer to video capture and display
peripherals.

\section{Evaluation}
We propose to evaluate our results by having the user follow an object
around the screen on a known path. This would be our target. We can
then compute the gaze of the user from the observations by the
system. We can compute the error between the computed target from the
gaze of the user and the known target location. From these two
measurements we can compute the mean squared error between the target
and the computed gaze location. We hope to achieve results that are
within the size of the panels used for a keyboard on the screen. If
the error is within the size of a key, then we can determine what keys
a user is looking at, and even use the system as an input device.



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
